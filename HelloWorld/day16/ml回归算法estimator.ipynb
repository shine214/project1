{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-11T05:42:05.806986Z",
     "start_time": "2025-01-11T05:42:05.065127Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, LogisticRegression, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, classification_report, roc_auc_score\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 回归算法 1.线性回归（正规方程版） LinearRegression()",
   "id": "99c8ca44577d761f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "    正规方程通过二次型直接求解，速度快，适用于小型数据，大数据的话时间复杂度O(m3)太大了\n",
    "    正规方程版线性回归就是 y = k1x1 + k2x2 + k3x3 + b\n",
    "    其中x是每个特征的参数值，k是每个特征对于结果的权重，b是偏置值，y即为最终的预测目标\n",
    "    这里使用b就是为了防止该函数被迫通过原点（因为在某些情况不见得参数为0结果就是0）\n",
    "    \n",
    "    它内部找最小值的办法\n",
    "    线性回归有一个直接求解的方法，称为 正规方程（Normal Equation）（二次型）。通过数学推导，它可以直接找到使 MSE 最小化的参数：\n",
    "    \n",
    "    \n",
    "    评估线性回归的指标：MSE\n",
    "\"\"\""
   ],
   "id": "4946b4ec6fad0d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:45:08.361908Z",
     "start_time": "2025-01-11T05:45:08.348832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "线性回归直接预测房子价格\n",
    ":return: None\n",
    "\"\"\"\n",
    "# 获取数据\n",
    "fe_cal = fetch_california_housing(data_home='../python_ml/data')\n",
    "\n",
    "print(\"获取特征值\")\n",
    "print(fe_cal.data.shape)\n",
    "print('-' * 50)\n",
    "print(fe_cal.data[0])\n",
    "print(\"目标值\")\n",
    "print(fe_cal.target) #单位是10万美金\n",
    "print(fe_cal.DESCR)\n",
    "print('-' * 50)\n",
    "print(fe_cal.feature_names) #特征列的名字"
   ],
   "id": "10f773179b9ef7bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取特征值\n",
      "(20640, 8)\n",
      "--------------------------------------------------\n",
      "[   8.3252       41.            6.98412698    1.02380952  322.\n",
      "    2.55555556   37.88       -122.23      ]\n",
      "目标值\n",
      "[4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. rubric:: References\n",
      "\n",
      "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "  Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "--------------------------------------------------\n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "标准化数据",
   "id": "a5ecf25ce7d81801"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T06:15:33.777124Z",
     "start_time": "2025-01-11T06:15:33.766035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(fe_cal.data,fe_cal.target,test_size = 0.25,random_state = 1)\n",
    "std = StandardScaler()\n",
    "x_train = std.fit_transform(x_train)\n",
    "x_test = std.transform(x_test)\n"
   ],
   "id": "597c5cb7c69af750",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "开始回归算法",
   "id": "36de0dd12a6ce87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T06:26:13.340845Z",
     "start_time": "2025-01-11T06:26:13.324476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(x_train,y_train)\n",
    "\n",
    "#y_predict返回的结果就是每个特征值对结果的影响因子的ndarray\n",
    "y_predict = lr.predict(x_test)\n",
    "print(y_test.shape) #目标值长度与y_predict长度相同\n",
    "print(y_predict)\n",
    "print(\"正规方程测试集里面每个房子的预测价格：\", y_predict[0:10])\n",
    "#下面是求测试集的损失，用均方误差，公式是(y_test-y_predict)^2/n\n",
    "print(\"正规方程的均方误差：\", mean_squared_error(y_test, y_predict))"
   ],
   "id": "2ff6818d5f2fe63d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5160,)\n",
      "[2.12391852 0.93825754 2.7088455  ... 1.24263061 2.73771901 1.75800594]\n",
      "正规方程测试集里面每个房子的预测价格： [2.12391852 0.93825754 2.7088455  1.70873764 2.82954754 3.50376456\n",
      " 3.0147162  1.62781292 1.74317518 2.01897806]\n",
      "正规方程的均方误差： 0.5356532845422556\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "保存模型",
   "id": "97ab9edc3e57657c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "    这个概念第一次涉及，齐用于存储之前训练模型所得的结果（说白了就是这些k值），当然之前的分类算法也可以进行保存\n",
    "    也就解开了如何持续训练模型的答案（增量训练）\n",
    "    \n",
    "    增量训练\n",
    "    不过决策树和随机森林不支持增量训练，只能将新数据和原有数据结合后重新训练\n",
    "    knn和朴素贝叶斯支持增量训练，但是knn由于数据量的增加计算效率会变得非常慢，所以增量会有限\n",
    "    通过   load_model.partial_fit(X_train2, y_train2) \n",
    "\"\"\""
   ],
   "id": "4217f110c5f24de6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "os.unlink('./tmp/test.pkl') # 删除之前的模型文件（这里只是学习如何删除模型文件）\n",
    "joblib.dump(lr, \"./tmp/test.pkl\") # 保存模型\n",
    "load_model = joblib.load('./tmp/test.pkl') #加载模型\n",
    "load_model.partial_fit(X_train2, y_train2) # 增量训练（如果该算法支持增量训练）"
   ],
   "id": "15492fce65e72a9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 回归算法 2.线性回归（梯度下降版） SGDRegressor()\n",
   "id": "1165152b083c57a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "    用梯度下降来实现线性回归，内部实现是通过迭代优化\n",
    "    对比正规方程版，速度慢，适用于大型数据\n",
    "\"\"\""
   ],
   "id": "cacf7d50730894d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T07:07:31.806771Z",
     "start_time": "2025-01-11T07:07:31.780592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sgd = SGDRegressor()\n",
    "\n",
    "#上面算法1已经处理完数据了，这里直接用\n",
    "sgd.fit(x_train,y_train) \n",
    "y_predict = sgd.predict(x_test)\n",
    "print(\"梯度下降的均方误差：\", mean_squared_error(y_test, y_predict))"
   ],
   "id": "c91ae6472b11d6b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "梯度下降的均方误差： 0.5392029356811116\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 模拟梯度下降的过程",
   "id": "95b37392b2cba05f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "    这段代码展示了一个典型的梯度下降优化过程，用来最小化一个简单的二次损失函数：\n",
    "    1.初始化参数W和α\n",
    "    2.迭代更新梯度，更新参数W\n",
    "    3.损失值L（W）随着迭代次数增加而减少，最终收敛到全局最优点（若是在二次函数中，他总能结束在全局最优）\n",
    "    \n",
    "\n",
    "\"\"\""
   ],
   "id": "46bdcf6ee75bcf60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T07:19:45.722577Z",
     "start_time": "2025-01-11T07:19:45.715711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w=1\n",
    "\n",
    "# alpha为学习率，它决定了每次更新的步长。如果学习率过大，可能导致参数跳跃过多，无法收敛；如果学习率过小，收敛速度会很慢。\n",
    "alpha=0.7\n",
    "\n",
    "#损失函数 L(w) \n",
    "def loss(w):\n",
    "    return 2*w**2+3*w+2\n",
    "#导数，也就是L(w)对w的梯度\n",
    "def dao_shu(w):\n",
    "    return 4*w+3\n",
    "\n",
    "#通过w = w - a*梯度 更新w\n",
    "for i in range(30):\n",
    "    \n",
    "    w=w-alpha*dao_shu(w)\n",
    "    print(f'w {w} 损失{loss(w)}')"
   ],
   "id": "85b61903ffc37bda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w -3.8999999999999995 损失20.71999999999999\n",
      "w 4.919999999999999 损失65.17279999999998\n",
      "w -10.955999999999996 损失209.19987199999983\n",
      "w 17.620799999999992 损失675.8475852799994\n",
      "w -33.817439999999976 损失2187.786176307197\n",
      "w 58.77139199999995 损失7086.4672112353155\n",
      "w -107.8885055999999 损失22958.193764402422\n",
      "w 192.09931007999978 损失74382.58779666382\n",
      "w -347.87875814399956 损失240997.6244611907\n",
      "w 624.0817646591992 损失780830.3432542577\n",
      "w -1125.4471763865586 损失2529888.352143795\n",
      "w 2023.7049174958051 损失8196836.300945894\n",
      "w -3644.768851492449 损失26557747.65506469\n",
      "w 6558.483932686408 损失86047100.4424096\n",
      "w -11807.371078835533 损失278792603.4734071\n",
      "w 21251.167941903957 损失903288033.2938386\n",
      "w -38254.202295427116 损失2926653225.912036\n",
      "w 68855.4641317688 损失9482356449.994993\n",
      "w -123941.93543718383 损失30722834896.023777\n",
      "w 223093.38378693088 损失99541985061.15703\n",
      "w -401570.1908164755 损失322516031596.1886\n",
      "w 722824.2434696557 损失1044951942369.6908\n",
      "w -1301085.73824538 损失3385644293275.837\n",
      "w 2341952.228841684 损失10969487510211.748\n",
      "w -4215516.1119150305 损失35541139533084.09\n",
      "w 7587926.901447055 损失115153292087190.52\n",
      "w -13658270.5226047 损失373096666362495.4\n",
      "w 24584884.840688456 损失1208833199014482.5\n",
      "w -44252794.81323922 损失3916619564806921.0\n",
      "w 79655028.56383058 损失1.268984738997442e+16\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 回归算法 3.岭回归 Ridge()",
   "id": "7b83636865ebf714"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "    岭回归是一种线性回归的扩展，主要用于解决线性回归模型中 多重共线性（即特征之间高度相关）的问题，同时可以抑制模型的过拟合。\n",
    "    \n",
    "    岭回归的目标是引入 L2 正则化 项，修改普通线性回归的损失函数，添加一个对回归系数大小的惩罚项。\n",
    "        在普通的损失函数计算中添加一个L = L + λ*|| w ||2\n",
    "            λ     ：正则化强度\n",
    "        ｜｜w｜｜2 ：所有回归系数的平方和\n",
    "        \n",
    "        \n",
    "\"\"\""
   ],
   "id": "15711575bdeaf347"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T07:29:52.176355Z",
     "start_time": "2025-01-11T07:29:52.171841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rd = Ridge(alpha = 0.02)\n",
    "rd.fit(x_train,y_train)\n",
    "\n",
    "y_predict = rd.predict(x_test)\n",
    "print(rd.coef_)\n",
    "print(\"岭回归的均方误差：\", mean_squared_error(y_test, y_predict))\n"
   ],
   "id": "4cac1b90b3b49cc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83166963  0.12159681 -0.26758236  0.30983534 -0.00517992 -0.04040432\n",
      " -0.90735215 -0.88211025]\n",
      "[ 0.83166963  0.12159681 -0.26758236  0.30983534 -0.00517992 -0.04040432\n",
      " -0.90735215 -0.88211025]\n",
      "岭回归的均方误差： 0.5356531179270403\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### L1正则化 Lasso回归\n",
   "id": "8c5667a14e647e96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "    lasso 回归就是L1正则化，也就是 在普通的损失函数计算中添加一个L = L + λ*|| w ||\n",
    "    也就是岭回归去掉平方\n",
    "    这样做可以实现特征选择：当某些特征对目标值的影响较小时，其对应的回归系数会被直接压缩到 0，从而实现 特征选择。\n",
    "    \n",
    "    优点：1.特征选择能力：Lasso 回归可以自动将不重要的特征对应的回归系数压缩到 0，从而实现特征选择。\n",
    "         2.抑制多重共线性：L1 正则化可以限制特征之间的共线性对模型的影响。\n",
    "         3.简单易用：与普通线性回归相比，Lasso 只增加了一个正则化参数 λ，但增强了模型的泛化能力。\n",
    "    缺点：1.处理特征多于样本时的不足：如果特征数多于样本数（例如高维数据），Lasso 可能会随机选择部分特征作为非零系数。\n",
    "         2.则化参数调试成本高：需要通过交叉验证等方法选择最佳 λ\n",
    "         3.不能很好地处理强相关特征：如果多个特征之间高度相关，Lasso 可能会随机选择其中一个特征而忽略其他特征。\n",
    "\"\"\""
   ],
   "id": "a7e90226111f53b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T07:41:17.982233Z",
     "start_time": "2025-01-11T07:41:17.965715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ls = Lasso(alpha = 0.001)\n",
    "\n",
    "ls.fit(x_train,y_train)\n",
    "y_predict = ls.predict(x_test)\n",
    "print(ls.coef_)\n",
    "print(\"Lasso回归的均方误差：\", mean_squared_error(y_test, y_predict))"
   ],
   "id": "b866f0c41431773c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.82655827  0.1225482  -0.25369194  0.29596304 -0.00381001 -0.03948424\n",
      " -0.89646842 -0.87060253]\n",
      "Lasso回归的均方误差： 0.5356324125105497\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
